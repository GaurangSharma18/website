<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Paper</title>
    <link rel="stylesheet" href="assets/css/jquery-ui.min.css">
    <link rel="stylesheet" href="assets/css/bootstrap.min.css">
    <link rel="stylesheet" href="assets/css/custom.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
    <main>
        <section class="paperBody">
            <div class="paperTitle">Multi-label Annotation for Visual Multi-Task Learning Models</div>
            <div class="paperAuthors">
                <span class="author"><a href="https://gaurangsharma18.github.io/website/official/index.html" target="_blank">Gaurang Sharma</a><sup class="authorship">1</sup>,</span>
                <span class="author"><a href="https://scholar.google.com/citations?user=B9poInQAAAAJ&hl=en" target="_blank">Alexandre Angleraud</a><sup class="authorship">1</sup></span>
                <span> and</span>
                <span class="author"><a href="https://scholar.google.com/citations?user=YYKF6BYAAAAJ&hl=en" target="_blank">Roel Pieters</a><sup class="authorship">1</sup></span>
            </div>
            <div class="paperLinks">
                <div class="heroButtons">
                    <a href="#">
                        <div class="buttonIcons"><svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true"
                                focusable="false" data-prefix="fas" data-icon="file-pdf" role="img"
                                xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg="">
                                <path fill="currentColor"
                                    d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z">
                                </path>
                            </svg></div>
                        <div class="buttonTitle">Paper</div>
                    </a>
                </div>
                <div class="heroButtons">
                    <a href="#">
                        <div class="buttonIcons icon"><svg class="svg-inline--fa fa-youtube fa-w-18" aria-hidden="true"
                                focusable="false" data-prefix="fab" data-icon="youtube" role="img"
                                xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" data-fa-i2svg="">
                                <path fill="currentColor"
                                    d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z">
                                </path>
                            </svg></div>
                        <div class="buttonTitle">Video</div>
                    </a>
                </div>
                <div class="heroButtons">
                    <a href="https://github.com/GaurangSharma18/Detection-Segmentation-and-Feature-estimation-pipeline" target="_blank">
                        <div class="buttonIcons icon"><svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true"
                                focusable="false" data-prefix="fab" data-icon="github" role="img"
                                xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg="">
                                <path fill="currentColor"
                                    d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z">
                                </path>
                            </svg></div>
                        <div class="buttonTitle">Code</div>
                    </a>
                </div>
                <!--div class="page-content heroButtons" id="sidebarCollapse">
                   
                    <a href="#">
                        <div class="buttonTitle">Details</div>
                    </a>
                </div-->
    
            </div>

            <div class="detailsWrapper">
                <!-- Vertical navbar -->
                <div class="vertical-nav toggleNav active" id="sidebar">
                  <div class="navMenuIconWrapper"><a class="icon">
                    <i class="fa fa-bars"></i>
                  </a></div>
                <div class="navList">
                    <div class="navListItem abstractHead">
                        Abstract
                    </div>
                    <div class="navListItem contributionHead">
                        Introduction
                    </div>
                    <div class="navListItem probStatHead">
                       Related Work
                    </div>
                    <div class="navListItem methodHead">
                        Methodology
                    </div>
                    <div class="navListItem resultHead">
                        Results
                    </div>
                    <div class="navListItem refrencesHead">
                        Refrences
                    </div>
                </div>
                
                </div>
                <div class="contentWrapper active" id="content">

                    <div class="paperSectionWrapper">
                        <div class="abstract">
                            <div class="paperHeadings">Abstract</div>
                            <div class="headDesc">Deep learning requires large amounts of data, and a well-defined pipeline for labeling and augmentation. Current solutions support numerous computer vision tasks with dedicated annotation types and formats, such as bounding boxes, polygons, and key points. These annotations can be combined into a single data format to benefit approaches such as multi-task models. However, 
                                to our knowledge, no available labeling tool supports the export functionality for a combined benchmark format, and no augmentation library supports transformations for the combination of all. In this work, these functionalities are presented, with visual data annotation and augmentation to train a multi-task model (object detection, segmentation, and key point extraction). The tools are demonstrated in two robot perception use cases.
                            </div>
                        </div>


                        <div class="Introduction d-none">
                            <div class="paperHeadings">Introduction</div>
                            <div class="headDesc">
                                <div class="motivationImageWrapper WrapText"><img class="motivationImage" src="assets/images/motivation.png"></div>
                                Advancements in deep learning have helped to address
                                numerous problems in different domains (e.g., robotics [1],
                                medicine [2] and agriculture [3]). Perception in particular
                                has been of major focus, where single purpose models
                                have been optimized and fine-tuned for one specific task at
                                hand [4]. More recently also multi-task models have been
                                developed [5], [6], where a shared model concurrently learns
                                multiple tasks. A fundamental aspect of these models is to
                                increase data efficiency, reduce overfitting, and enable quick
                                learning due to the use of auxiliary information. They use a
                                unified single backbone structure having multiple descriptive
                                heads to address technical computer vision challenges like
                                boundary detection, semantic segmentation, object detection, etc. To effectively utilize multi-task models, multilabel annotations should be supported by the framework.
                                This implies that images and the objects inside them can
                                be annotated with multiple different labels (e.g., bounding
                                boxes, polygons and key points), to be used for learning
                                different tasks and thus generate multiple outputs. Compared
                                to single-task models this is less computationally expensive
                                and requires less annotated records. In addition, multiple
                                outputs per image or object provides additional information
                                compared to single-task models. Fig. 1 depicts this visually
                                in an object detection problem. Two objects are annotated
                                and correctly detected, however, in this context, the lower
                                object is incorrectly placed. From detection or segmentation
                                alone, this cannot be determined, yet with the estimated
                                key points as additional information (i.e., three blue points),
                                incorrect placement can be detected. Existing labeling and
                                annotation solutions [7] do not provide the combined annotation export functionality, which results in solutions that
                                combine several export formats to accomplish a common
                                task. This served as motivation for the development of a novel data pipeline to provide domain-specific annotation and
                                augmentation strategies. Best-of-breed libraries such as Albumentation [8] and Label Studio [9] are integrated to create
                                a generic data generation solution for training deep learning
                                models. Our developments are evaluated by applications in
                                object detection, object configuration estimation and depth
                                estimation. In this work, our contributions are:
                                <ol>
                                    <li>A novel pipeline that enables labeling of image
                                        datasets with different annotations and formats</li>
                                    <li>The augmentation of this dataset without conversion to
                                        different formats</li>
                                    <li>The validation of results in two industrial use cases
                                        with different computer vision tasks</li>
                                  </ol> 
                            </div>
                           
                        </div>

                        <div class="RelatedWork d-none">
                            <div class="paperHeadings">Related Work</div>
                            <div class="headDesc">
                                <div id="accordionRelatedWork" class="accordion">
                                    <div class="card">
                                      <div class="card-header" id="headingOneRelatedWork">
                                        <h5 class="mb-0">
                                          <button class="btn btn-link" data-toggle="collapse" data-target="#collapseOneRelatedWork" aria-expanded="true" aria-controls="collapseOneRelatedWork">
                                            Multi-task Learning
                                          </button>
                                        </h5>
                                      </div>
                                  
                                      <div id="collapseOneRelatedWork" class="collapse show" aria-labelledby="headingOneRelatedWork" data-parent="#accordionRelatedWork">
                                        <div class="card-body">
                                            Multi-task learning [5] in the field of computer vision
                                            [6] has shown recent success with methods such as CrossStitch [18] and UberNet [19]. As comparison, single-task
                                            models are more expensive to train and require a larger
                                            amount of annotated data than multi-task models. Moreover,
                                            knowing which tasks should be trained together could aid
                                            in improving prediction quality and acting as a preventative
                                            measure against the occurrence of negative transfer [20].
                                            Performance improvement of a specific task can thus lead
                                            to performance degradation of other tasks [21]. Currently,
                                            libraries such as YOLOR [22] and YOLOv7 [11] provide
                                            configuration features to facilitate multi-task learning.
                                        </div>
                                      </div>
                                    </div>
                                    <div class="card">
                                      <div class="card-header" id="headingTwoRelatedWork">
                                        <h5 class="mb-0">
                                          <button class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseTwoRelatedWork" aria-expanded="false" aria-controls="collapseTwoRelatedWork">
                                            Image Annotation and Storing Formats
                                          </button>
                                        </h5>
                                      </div>
                                      <div id="collapseTwoRelatedWork" class="collapse" aria-labelledby="headingTwoRelatedWork" data-parent="#accordionRelatedWork">
                                        <div class="card-body">
                                            Labeling data is a critical component for deep learning
                                            as it provides the ground truth for estimating errors. Incorrectly labeled datasets lead to poor training and higher
                                            validation losses [23]. Nowadays, there are several machine
                                            learning based tools available that speed up the automatic
                                            labeling process and reduce human labor and time [24].
                                            MS COCO [10], PASCAL VOC [12], and YOLO are some
                                            commonly used benchmarks with different storage format
                                            styles. MS COCO is a rich (.json) format that offers a large
                                            variety of annotation storing structures. PASCAL VOC, on
                                            the other hand, supports a (.xml) structure that is useful
                                            for detection and segmentation tasks but has limitations in
                                            supporting key points. YOLO models accept (.txt) file format
                                            for detection tasks, however, YOLOv7 [11] accepts polygon
                                            segmentation and key point representations as well. While
                                            numerous labeling tools are available, each provides support
                                            for different export formats and annotation types (see Table
                                            I). For example, only a few tools offer direct labeling options
                                            for polygon segmentation, bounding boxes and key points
                                            simultaneously. And when the opportunity is provided, only
                                            a combined JSON export format is available, thereby not
                                            directly supporting state-of-the-art deep learning libraries. As
                                            a result, custom data loaders need to be developed to enable
                                            library-specific data formats.
                                            
                                            <div class="accordianImageWrapper"><img class="accordianImage" src="assets/images/SurveyTable.png" alt="Annotation Tools Survey Table"/> </div>
                                            
                                        </div>
                                      </div>
                                    </div>
                                    <div class="card">
                                      <div class="card-header" id="headingThreeRelatedWork">
                                        <h5 class="mb-0">
                                          <button class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseThreeRelatedWork" aria-expanded="false" aria-controls="collapseThreeRelatedWork">
                                            Image Augmentation
                                          </button>
                                        </h5>
                                      </div>
                                      <div id="collapseThreeRelatedWork" class="collapse" aria-labelledby="headingThreeRelatedWork" data-parent="#accordionRelatedWork">
                                        <div class="card-body">
                                            Data augmentation is a form of data expansion that can
                                            improve model performance by assisting in its generalization,
                                            robustness and convergence. While augmentation is a wellknown approach, libraries such as TensorFlow [25] and
                                            PyTorch [26] provide very few transformation strategies.
                                            Albumentations [8], on the other hand provides around six different libraries to apply a broad range of transformations
                                            within a single tool. However, if custom and complex transformations are required, then libraries like OpenCV [27] can
                                            be utilized for basic image manipulation algorithms. Smart
                                            augmentation techniques also exist [28], in which suitable
                                            augmentations are learned during the process of training a
                                            deep neural network
                                        </div>
                                      </div>
                                    </div>
                                    <div class="card">
                                        <div class="card-header" id="headingFourRelatedWork">
                                          <h5 class="mb-0">
                                            <button class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseFourRelatedWork" aria-expanded="false" aria-controls="collapseFourRelatedWork">
                                                Object detection and Pose estimation
                                            </button>
                                          </h5>
                                        </div>
                                        <div id="collapseFourRelatedWork" class="collapse" aria-labelledby="headingFourRelatedWork" data-parent="#accordionRelatedWork">
                                          <div class="card-body">
                                            Traditionally, keypoint estimation, segmentation, and object detection were considered separate problems. The solution for object detection evolved in the last 20 years
                                            from hand-crafted features to deformable transformers [29].
                                            Research on segmentation started with thresholding and later
                                            led to methods such as semantic, instance, and panoptic
                                            segmentation. On the other hand, local features, called keypoints, became popular through corner and edge detectors
                                            [30], and today there are several ways to predict them,
                                            including Faster R-CNN [31], which is based on local
                                            keypoints. In the field of robotics and computer vision, segmentation masks are used to predict keypoints, as presented
                                            in [32]. Recently, combinations of both to predict human and
                                            object poses is addressed by [33]. In this, COCO is used as
                                            the primary format, emphasizing its relevance.
                                          </div>
                                        </div>
                                      </div>
                                  </div>
                            </div>
                        </div>

                        <div class="Methodology d-none">
                            <div class="paperHeadings">Methodology</div>
                            <div class="headDesc">
                                In this section, we present our methodology which enables
                                the annotation of images with multiple labels in multiple
                                formats and the export of these in a generic format, for
                                training a multi-task detection model.

                                <div id="accordionMethodology" class="accordion">
                                    <div class="card">
                                        <div class="card-header" id="headingOneMethodology">
                                          <h5 class="mb-0">
                                            <button class="btn btn-link" data-toggle="collapse" data-target="#collapseOneMethodology" aria-expanded="true" aria-controls="collapseOneMethodology">
                                                Architecture
                                            </button>
                                          </h5>
                                        </div>
                                    
                                        <div id="collapseOneMethodology" class="collapse show" aria-labelledby="headingOneMethodology" data-parent="#accordionMethodology">
                                          <div class="card-body">
                                            The multi-task architecture of ResNet-50 FPN Keypoint
                                            R-CNN [34] supports the combined data annotations and is
                                            utilized for training the detection model (see Fig. 2). The
                                            output of the model contains all three detection formats, for
                                            object recognition, segmentation and feature extraction at the
                                            same time
                                          </div>
                                          <div class="accordianImageWrapper"><img class="accordianImage" src="assets/images/architecture.png" alt="Multitask model architecture"/> </div>
                                        </div>
                                      </div>
                                      <div class="card">
                                        <div class="card-header" id="headingTwoMethodology">
                                          <h5 class="mb-0">
                                            <button class="btn btn-link" data-toggle="collapse" data-target="#collapseTwoMethodology" aria-expanded="false" aria-controls="collapseTwoMethodology">
                                                Data Annotation
                                            </button>
                                          </h5>
                                        </div>
                                    
                                        <div id="collapseTwoMethodology" class="collapse" aria-labelledby="headingTwoMethodology" data-parent="#accordionMethodology">
                                          <div class="card-body">
                                            Our pipeline starts with the image annotation of polygons,
                                            key points and bounding boxes in Label Studio. As Label
                                            Studio does not provide the functionality for simultaneously annotating features for polygons and key points, our
                                            approach updates polygon configurations by adding a key
                                            points configuration script to the user interface, maintaining
                                            a single image via the 'toName' attribute. Bounding boxes
                                            are extracted from the polygon annotations by selecting the
                                            outer bounds of the polygon coordinates. Following, all annotations are exported into a JSON-min format and converted
                                            into COCO format, containing all three data annotation types
                                            in a single output format.
                                          </div>
                                        </div>
                                      </div>
                                      <div class="card">
                                        <div class="card-header" id="headingThreeMethodology">
                                          <h5 class="mb-0">
                                            <button class="btn btn-link" data-toggle="collapse" data-target="#collapseThreeMethodology" aria-expanded="false" aria-controls="collapseThreeMethodology">
                                                Data Augmentation
                                            </button>
                                          </h5>
                                        </div>
                                    
                                        <div id="collapseThreeMethodology" class="collapse" aria-labelledby="headingThreeMethodology" data-parent="#accordionMethodology">
                                          <div class="card-body">
                                            Tools such as Albumentations [8] offer a wide range of
                                            image transformations. However, it does not offer augmentation support for polygon and run-length encoding COCO
                                            formats, limiting the support for an integrated augmentation
                                            strategy. To enable the augmentation of multi-label annotated
                                            data, such as bounding boxes, key points and COCO segmentation formats, the following steps are performed. First, the
                                            combined data (in COCO format) is loaded and extracted
                                            into separate arrays, such as bounding boxes, key points,
                                            area, etc. Second, polygons are converted to key points
                                            and appended to a key point array. Third, the conversion
                                            of (x,y) key points to (x,y,v) is done to maintain COCO
                                            standards, followed by appending invisible key points to
                                            keep the number of labeled key points the same. Dataset
                                            augmentation is then performed using Albumentation’s key
                                            point transformation strategy, after which the key points
                                            are converted back to polygons. As a result, this procedure
                                            enables the transformation of COCO polygon format by the
                                            Albumentation library
                                          </div>
                                        </div>
                                      </div>
                                </div>
                            </div>
                        </div>

                        <div class="Results d-none">
                            <div class="paperHeadings">Results</div>
                            <div class="headDesc">
                                <div id="accordionResults" class="accordion">
                                    <div class="card">
                                      <div class="card-header" id="headingOneResults">
                                        <h5 class="mb-0">
                                          <button class="btn btn-link" data-toggle="collapse" data-target="#collapseOneResults" aria-expanded="true" aria-controls="collapseOneResults">
                                            Practical Use Cases and System Integration
                                          </button>
                                        </h5>
                                      </div>
                                  
                                      <div id="collapseOneResults" class="collapse show" aria-labelledby="headingOneResults" data-parent="#accordionResults">
                                        <div class="card-body">
                                            The use cases aim to detect different objects for automation purposes. Two assembly sets are selected including a
                                            diesel engine and a planetary gearbox, and their internal
                                            parts (e.g., rocker arms, pushrods, bolts, gears, housing,
                                            etc.). The diesel engine [35] and planetary gearbox [36]
                                            datasets contain 195 and 150 RGB images, respectively,
                                            manually annotated by drawing polygons and labeling key
                                            points (see Fig. 3a and 3c). Augmentation is then performed
                                            on the annotated data by different transformation strategies,
                                            completing datasets of 280,000 and 170,000 images. Training
                                            of the multi-task models followed typical training and evaluation steps [34] with hyper-parameters such as momentum,
                                            weight decay, and regularization to achieve loss convergence
                                            in about 2000 epochs. The images were captured with an
                                            Intel Realsense D435 camera. All tools are open-source
                                            available from the dataset documentations [35], [36].
                                        </div>
                                      </div>
                                    </div>
                                    <div class="card">
                                        <div class="card-header" id="headingTwoResults">
                                          <h5 class="mb-0">
                                            <button class="btn btn-link" data-toggle="collapse" data-target="#collapseTwoResults" aria-expanded="false" aria-controls="collapseTwoResults">
                                                Object and Key point Detection
                                            </button>
                                          </h5>
                                        </div>
                                    
                                        <div id="collapseTwoResults" class="collapse" aria-labelledby="headingTwoResults" data-parent="#accordionResults">
                                          <div class="card-body">
                                            Fig. 3b and 3d present the output of the trained multitask models on the two use cases with results for object
                                            detection and segmentation, and key point estimations. This
                                            demonstrates that a single multi-task model can utilize multiple data annotation formats as part of the dataset, train a
                                            model and achieve successful detection outputs. On average,
                                            the detection confidence is over 90 percent. Furthermore, as
                                            main beneficial outcome Table II presents that the achieved
                                            frame rate of the multi-task model is higher than running
                                            multiple models in parallel. For high-resolution camera input
                                            (1280×720), the difference is almost double, i.e., 8.7 FPS
                                            versus 4.8 FPS, respectively. Likewise, the memory requirements for a multi-task model is less than running multiple
                                            single-task models in parallel (i.e., 500 MB vs 700 MB).
                                          </div>
                                          <div class="accordianImageWrapper"><img class="accordianImage resultTable" src="assets/images/resultTable.png" alt="Annotation Tools Survey Table"/> </div>
                                            
                                        </div>
                                      </div>
                                      <div class="card">
                                        <div class="card-header" id="headingThreeResults">
                                          <h5 class="mb-0">
                                            <button class="btn btn-link" data-toggle="collapse" data-target="#collapseThreeResults" aria-expanded="false" aria-controls="collapseThreeResults">
                                                Object Configuration and Depth Estimation
                                            </button>
                                          </h5>
                                        </div>
                                    
                                        <div id="collapseThreeResults" class="collapse" aria-labelledby="headingThreeResults" data-parent="#accordionResults">
                                          <div class="card-body">
                                            Two further functionalities are developed to demonstrate
                                            the use of multiple detection format outputs from the multitask model. These are object configuration and depth estimation, based on the object segmentation masks and key points,
                                            respectively. Segmentation masks generated by the multi-task
                                            model can be used to determine rocker arm orientations with
                                            respect to the image, with the 2nd order moment of the mask. 
                                            The three estimated key points on the rocker arm then verify
                                            correct configuration of the rocker arm object with respect
                                            to the engine. The process for finding the outer side for the
                                            correct configuration is to minimize the axis of the smallest
                                            2nd order moment, then apply a cross-product between the
                                            axis coordinates and predicted key points. The one key point
                                            with an opposite sign to the others indicates the outer side.
                                            Fig. 1 illustrates the correct orientation of the rocker arm
                                            on the engine and verifies (in)correct placement.
                                            To estimate the depth of the rocker arm target, with respect
                                            to the camera, two methods were considered: by segmentation mask or by key points (see Fig. 3b). In both cases the
                                            depth value from corresponding RGB pixel coordinates are
                                            retrieved from a depth camera (Intel Realsense D435). For
                                            a segmentation mask, the entire mask area as depth range
                                            is taken and an average depth value is returned, and for a key point, a single depth value is returned. The key point
                                            approach provides 10 times less difference in depth range
                                            (i.e., 468 ± 5mm) as compared to the segmentation mask,
                                            and is therefore found to be more reliable.
                                          </div>
                                        </div>
                                      </div>
                                      <div class="card">
                                        <div class="card-header" id="headingFourResults">
                                          <h5 class="mb-0">
                                            <button class="btn btn-link" data-toggle="collapse" data-target="#collapseFourResults" aria-expanded="false" aria-controls="collapseFourResults">
                                                Discussion
                                            </button>
                                          </h5>
                                        </div>
                                    
                                        <div id="collapseFourResults" class="collapse" aria-labelledby="headingFourResults" data-parent="#accordionResults">
                                          <div class="card-body">
                                            The proposed approach enables images to be annotated by
                                            multiple different labels, including bounding boxes, polygons
                                            and key points, during a single annotation step, and store the
                                            result in a single convenient format (COCO). The annotations
                                            are then all included in the same data augmentation process
                                            to collect a dataset utilized as input for training a multi-task
                                            model. In effect, this provides a single model with multiple
                                            detection outputs in different detection formats (bounding
                                            box, segmentation masks and key points). Compared to
                                            single-task models that only consider single annotation format input and detection output, the following benefits are
                                            identified. 1. Time-saving for image annotation and augmentation. All annotations can be done in a single image,
                                            without changing any formats for different annotation types.
                                            For example, in case of a single task-model, one image would
                                            need to be annotated three times separately for three different
                                            annotation types. 2. Memory space requirements saving in
                                            model training. As only a single multi-task model needs to
                                            be trained instead of multiple single-task models, size of the
                                            model is reduced. 3. Combining detection outputs. Detection modalities from the multi-task model can be combined to
                                            solve additional detection tasks, not directly provided by the
                                            model. For example, depth and object configuration from key
                                            points as shown by our use cases, or other post-prediction
                                            applications such as boundary or grasp pose detection, or the
                                            estimation of object properties.
                                            
                                            A current limitation of the annotation pipeline is that it
                                            relies on existing annotation (Label Studio [9]) and augmentation (Albumentations [8]) tools. While these tools can be
                                            used under an open source license, access and support might
                                            change over time. Another limitation is that, still, annotation
                                            is a time-consuming step in the process of generating a
                                            learning-based detection model. Selection of which data to
                                            annotate and in which format (e.g., depth from pixel masks
                                            or key points) requires manual decisions and an iterative
                                            process to select a best outcome for the tasks. Automated
                                            and interactive annotation (e.g., based on machine learning
                                            [23], [37]) and semi-automated, interactive image annotation
                                            for visual detection exist [38], but are still in early stages of
                                            research. This is a natural next-step in developing assistive
                                            tools for model training and planned as future work.
                                          </div>
                                        </div>
                                      </div>
                                </div>
                            </div>
                        </div>
                        <div class="refrences d-none">
                            <div class="paperHeadings">References</div>
                            <div class="headDesc">
                              <div class="reference">[1] A. I. Karoly ´ et al., “Deep learning in robotics: Survey on model
                                structures and training strategies,” IEEE Transactions on Systems,
                                Man, and Cybernetics: Systems, vol. 51, no. 1, pp. 266–279, 2021.</div>
                              <div class="reference">[2] F. Piccialli et al., “A survey on deep learning in medicine: Why, how
                                and when?,” Information Fusion, vol. 66, pp. 111–137, 2021.</div>
                              <div class="reference">[3] A. Kamilaris and F. X. Prenafeta-Boldu, “Deep learning in agriculture: ´
                                A survey,” Computers and electronics in agriculture, vol. 147, pp. 70–
                                90, 2018.</div>
                              <div class="reference">[4] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
                                with deep convolutional neural networks,” Communications of the
                                ACM, vol. 60, no. 6, pp. 84–90, 2017.</div>
                              <div class="reference">[5] R. Caruana, “Multitask learning,” Machine Learning, vol. 28, pp. 41–
                                75, 1997.</div>
                              <div class="reference">[6] S. Vandenhende et al., “Multi-task learning for dense prediction
                                tasks: A survey,” IEEE Transactions on Pattern Analysis and Machine
                                Intelligence, vol. 44, no. 7, pp. 3614–3633, 2022.</div>
                              <div class="reference">[7] B. Pande et al., “A review of image annotation tools for object detection,” in International Conference on Applied Artificial Intelligence
                                and Computing, pp. 976–982, 2022.</div>
                              <div class="reference">[8] Albumentations. https://albumentations.ai/, 2023.</div>
                              <div class="reference">[9] Label Studio. https://labelstud.io/, 2023.</div>
                              <div class="reference">[10] T.-Y. Lin et al., “Microsoft COCO: Common objects in context,” in
                                European Conf. on Computer Vision (ECCV), pp. 740–755, 2014.</div>
                              <div class="reference">[11] C.-Y. Wang et al., “Yolov7: Trainable bag-of-freebies sets new
                                state-of-the-art for real-time object detectors,” arXiv preprint
                                arXiv:2207.02696, 2022.</div>
                              <div class="reference">[12] M. Everingham, L. V. Gool, C. K. Williams, J. Winn, and A. Zisserman, “The Pascal Visual Object Classes (VOC) challenge,” International Journal of Computer Vision, vol. 88, pp. 303–338, 6 2010.
                              </div>
                              <div class="reference">[13] V7Labs. https://www.v7labs.com/, 2023.</div>
                              <div class="reference">[14] CVAT. https://www.cvat.ai/, 2023.</div>
                              <div class="reference">[15] P. Voigtlaender et al., “MOTS: Multi-object tracking and segmentation,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
                              </div>
                              <div class="reference">[16] Labelbox. https://labelbox.com, 2023.</div>
                              <div class="reference">[17] LabelMe. http://labelme.csail.mit.edu, 2023.</div>
                              <div class="reference">[18] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert, “Cross-stitch
                                networks for multi-task learning,” in IEEE Conference on Computer
                                Vision and Pattern Recognition (CVPR), pp. 3994–4003, 2016.</div>
                              <div class="reference">[19] I. Kokkinos, “UberNet: Training a universal convolutional neural
                                network for low-, mid-, and high-level vision using diverse datasets
                                and limited memory,” in IEEE Conference on Computer Vision and
                                Pattern Recognition (CVPR), pp. 6129–6138, 2017.</div>
                              <div class="reference">[20] C. Fifty, E. Amid, Z. Zhao, T. Yu, R. Anil, and C. Finn, “Efficiently
                                identifying task groupings for multi-task learning,” Advances in Neural
                                Information Processing Systems, vol. 34, pp. 27503–27516, 2021.</div>
                              <div class="reference">[21] Z. Meng, X. Yao, and L. Sun, “Multi-task distillation: Towards mitigating the negative transfer in multi-task learning,” in IEEE International
                                Conference on Image Processing (ICIP), pp. 389–393, 2021.</div>
                              <div class="reference">[22] C.-Y. Wang, I.-H. Yeh, and H.-Y. M. Liao, “You Only Learn One
                                Representation: Unified network for multiple tasks,” arXiv preprint
                                arXiv:2105.04206, 2021.</div>
                              <div class="reference">[23] P. Bhagat and P. Choudhary, “Image annotation: Then and now,” Image
                                and Vision Computing, vol. 80, pp. 1–23, 2018.</div>
                              <div class="reference">[24] C. Sager, C. Janiesch, and P. Zschech, “A survey of image labelling for
                                computer vision applications,” Journal of Business Analytics, vol. 4,
                                no. 2, pp. 91–110, 2021.</div>
                              <div class="reference">[25] M. Abadi et al., “TensorFlow: Large-scale machine learning on
                                heterogeneous systems,” 2015. Software available from tensorflow.org.</div>
                              <div class="reference">[26] A. Paszke et al., “PyTorch: An imperative style, high-performance
                                deep learning library,” Advances in neural information processing
                                systems, vol. 32, 2019.</div>
                              <div class="reference">[27] G. Bradski, “The OpenCV Library,” Dr. Dobb’s Journal of Software
                                Tools, 2000.</div>
                              <div class="reference">[28] J. Lemley, S. Bazrafkan, and P. Corcoran, “Smart augmentation
                                learning an optimal data augmentation strategy,” IEEE Access, vol. 5,
                                pp. 5858–5869, 2017.</div>
                              <div class="reference">[29] Z. Zou et al., “Object detection in 20 years: A survey,” Proceedings
                                of the IEEE, vol. 111, no. 3, pp. 257–276, 2023.</div>
                                <div class="reference">[30] C. Harris, M. Stephens, et al., “A combined corner and edge detector,”
                                  in Alvey vision conference, vol. 15, pp. 10–5244, 1988.</div>
                                <div class="reference">[31] X. Ding et al., “Local keypoint-based Faster R-CNN,” Applied Intelligence, vol. 50, pp. 3007–3022, 10 2020.</div>
                                <div class="reference">[32] C.-C. Wong et al., “Manipulation planning for object re-orientation
                                  based on semantic segmentation keypoint detection,” Sensors, vol. 21,
                                  no. 7, p. 2280, 2021.</div>
                                <div class="reference">[33] L. Manuelli, W. Gao, P. Florence, and R. Tedrake, “kPAM: KeyPoint
                                  Affordances for Category-Level Robotic Manipulation,” in The International Symposium of Robotics Research, pp. 132–157, 2019.</div>
                                <div class="reference">[34] Detectron2. https://github.com/facebookresearch/
                                  detectron2, 2023.</div>
                                <div class="reference">[35] G. Sharma, R. Pieters, and A. Angleraud, “Engine assembly
                                  dataset.” http://dx.doi.org/10.5281/zenodo.7669593,
                                  Feb. 2023.</div>
                                <div class="reference">[36] G. Sharma and R. Pieters, “Planetary gear dataset.” http://dx.
                                  doi.org/10.5281/zenodo.8223473, Aug. 2023.</div>
                                <div class="reference">[37] Q. Cheng et al., “A survey and analysis on automatic image annotation,” Pattern Recognition, vol. 79, pp. 242–259, 2018.</div>
                                <div class="reference">[38] J. Bragantini, A. X. Falcao, and L. Najman, “Rethinking interactive ˜
                                  image segmentation: Feature space annotation,” Pattern Recognition,
                                  vol. 131, p. 108882, 2022.</div>
                            </div>
                        </div>
                    </div>
                    <!--div class="frontPageImages">
                        <div><img class="imageStyling" src="assets/images/bolts.gif"></div>
                        <div><img class="imageStyling" src="assets/images/rockerarm.gif"></div>
                        <div><img class="imageStyling" src="assets/images/pushrods.gif"></div>
                        
                    </div-->

                </div>
                <!-- Page content holder -->

            </div>
        </section>

    </main>
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery-ui.min.js"></script>
    <script src="assets/js/bootstrap.min.js"></script>
    <script src="assets/js/custom.js"></script>
</body>

</html>